experiment_name:
seed: 0
use_gpu: True
gpu_id: 0

environment: minigrid
apply_curriculum: True
    
minigrid:
    size: [40, 50]
    living_reward: 0
    no_op_penalty: 0
    starting_position: [0, 0] # random if not given
    num_rewards: 1
    reward_positions: [[38, 45]] # random if not given
    reward_magnitudes: [1.]
    repeat_rewards: False
    episode_timeout: 400 # infinity if not given

minigrid_curriculum:
    transition_episodes: [100]
    environment_changes: 
        - []

atari:
    atari_env_name: "Pong-v0"
    pre_processing:
        - max_over:
            num_frames: 2
        - gray_scale:
        - down_sample:
            width: 84
            height: 84

    frame_stack: 4
    frame_skip: 4
    episode_timeout: 200 # infinity if not given
    encoded_state_dimensions: [4, 84, 84]


learner: 
    type: q_learning
    learning_rate: 0.1
    discount_factor: 0.99 
    epsilon: 0.1
    initialisation: random
    visitation_penalty_type: hard_coded # hard_coded, function

    hard_coded: 
        vp_schedule:
            - [0, 0] # from step 0, 0 penalty

sarsa_lambda:
    trace_lambda: 0.05
    behaviour: epsilon_greedy
    target: greedy

q_learning:
    behaviour: epsilon_greedy
    target: greedy

dqn:
    batch_size: 32
    num_replay_fill_trajectories: 10000
    replay_buffer_size: 100000
    target_network_update_period: 50

    optimiser: adam

    layer_specifications:
        - conv:
            in_channels: 4
            num_filters: 32 
            kernel_size: 8
            stride: 4
            nonlinearity:  relu
        - conv:
            in_channels: 32
            num_filters: 64 
            kernel_size: 4
            stride: 2
            nonlinearity:  relu
        - conv:
            in_channels: 64
            num_filters: 64 
            kernel_size: 3
            stride: 1
            nonlinearity:  relu
        - flatten:
            nonlinearity: identity
        - fc:
            in_features: 3136
            out_features: 512
            nonlinearity: relu
        - fc:
            in_features: 512
            nonlinearity: identity

training:
    num_episodes: 100000
    test_frequency: 1
    testing: 
        - greedy
    full_test_log_frequency: 2500
    train_log_frequency: 2500

logging:
    print_frequency: 500
    checkpoint_frequency: 100
    columns:
        - train_episode_reward
        - train_episode_length
        - test_episode_reward
        - test_episode_length
        - no_repeat_test_episode_reward
        - no_repeat_test_episode_length
        - cycle_count
    arrays:
    plots:
        - value_function
        - plot_episode_lengths
        - plot_episode_rewards
        - no_repeat_test_episode_length
        - no_repeat_test_episode_reward
        - visitation_count_heatmap
        - individual_test_run
        - individual_train_run
        - individual_no_rep_test_run
        - individual_no_rep_train_run
        - cycle_count

post_processing:
    plot_tags:
        - train_episode_reward
        - train_episode_length
        - test_episode_reward
        - test_episode_length
        