experiment_name:
seed: 0

environment: minigrid
apply_curriculum: True
    
minigrid:
    size: [40, 50]
    living_reward: 0
    no_op_penalty: 0
    starting_position: [0, 0] # random if not given
    num_rewards: 1
    reward_positions: [[38, 45]] # random if not given
    reward_magnitudes: [1.]
    repeat_rewards: False
    episode_timeout: 400 # infinity if not given

minigrid_curriculum:
    transition_episodes: [100]
    environment_changes: 
        - [change_starting_position, [25, 2]]

learner: 
    type: sarsa_lambda
    learning_rate: 0.1
    discount_factor: 0.99 
    epsilon: 0.1
    visitation_penalty: 0
    initialisation: random

sarsa_lambda:
    trace_lambda: 0.05
    behaviour: epsilon_greedy
    target: greedy

q_learning:
    behaviour: epsilon_greedy
    target: greedy

training:
    num_episodes: 10000
    test_frequency: 1
    full_test_log_frequency: 2500
    train_log_frequency: 2500

logging:
    checkpoint_frequency: 100
    columns:
        - train_episode_reward
        - train_episode_length
        - test_episode_reward
        - test_episode_length
        - no_repeat_test_episode_reward
        - no_repeat_test_episode_length
        - cycle_count
    arrays:
    plots:
        - value_function
        - plot_episode_lengths
        - plot_episode_rewards
        - no_repeat_test_episode_length
        - no_repeat_test_episode_reward
        - visitation_count_heatmap
        - individual_test_run
        - individual_train_run
        - individual_no_rep_test_run
        - individual_no_rep_train_run
        - cycle_count

post_processing:
    plot_tags:
        - train_episode_reward
        - train_episode_length
        - test_episode_reward
        - test_episode_length
        