experiment_name:
seed: 0
use_gpu: True
gpu_id: 0

environment: multiroom
apply_curriculum: False
    
minigrid:
    size: [20, 20]
    living_reward: 0
    no_op_penalty: 0
    starting_position: [0, 0] # random if not given
    num_rewards: 1
    reward_positions: [[10, 10]] # random if not given
    reward_magnitudes: [1.]
    repeat_rewards: False
    episode_timeout: 200 # infinity if not given
    plot_origin: lower

minigrid_curriculum:
    transition_episodes: []
    environment_changes: 
        - []

multiroom:
    ascii_map_path: simple_map_1.txt
    episode_timeout: 400 # infinity if not given
    plot_origin: lower

atari:
    atari_env_name: "PongNoFrameskip-v4"

    implementation: wrapper

    pre_processing:
        - max_over:
            num_frames: 2
        - gray_scale:
        - down_sample:
            width: 84
            height: 84

    frame_stack: 4
    frame_skip: 4
    episode_timeout: 500 # infinity if not given
    encoded_state_dimensions: [4, 84, 84]
    plot_origin: upper


learner: 
    type: ensemble_q_learning
    learning_rate: 0.01
    gradient_momentum: 0.95
    squared_gradient_momentum: 0.95
    min_squared_gradient: 0.01
    discount_factor: 0.99 
    epsilon:
        schedule: constant

        constant:
            value: 0.1
        
        linear_decay:
            initial_value: 0.9
            final_value: 0.02
            anneal_duration: 100000
    
    initialisation: random

    # hard_coded, adaptive_uncertainty, adaptive_arriving_uncertainty, 
    # policy_entropy_penalty or potential_based_adaptive_uncertainty
    visitation_penalty_type: policy_entropy_penalty 

    hard_coded: 
        vp_schedule:
            - [0, -0.05] # from step 0, 0 penalty
        
    adaptive_uncertainty:
        action_function: mean # mean or max or select
        multiplicative_factor: 0.5

    adaptive_arriving_uncertainty:
        action_function: mean # mean or max
        multiplicative_factor: 0.5

    potential_based_adaptive_uncertainty:
        action_function: mean # mean or max or select
        multiplicative_factor: 0.5

    policy_entropy_penalty:
        multiplicative_factor: 0.5

sarsa_lambda:
    trace_lambda: 0.05
    behaviour: epsilon_greedy
    target: greedy

q_learning:
    behaviour: epsilon_greedy
    target: greedy

ensemble_q_learning:
    num_learners: 4
    copy_learner_initialisation: False
    behaviour: epsilon_greedy
    targets: 
        - greedy_sample
        - greedy_mean
        - greedy_vote
    parallelise_ensemble: False

ensemble_dqn:
    num_learners: 4
    copy_learner_initialisation: False
    behaviour: epsilon_greedy
    targets: 
    parallelise_ensemble: False

dqn:
    batch_size: 32
    num_replay_fill_trajectories: 500
    replay_buffer_size: 1000000
    target_network_update_period: 10000
    normalise_state: True
    gradient_clipping: [-1, 1]

    optimiser: rms_prop

    layer_specifications:
        - conv:
            in_channels: 4
            num_filters: 32 
            kernel_size: 8
            stride: 4
            nonlinearity:  relu
            bias_initialisation: 
            weight_initialisation: 
        - conv:
            in_channels: 32
            num_filters: 64 
            kernel_size: 4
            stride: 2
            nonlinearity:  relu
            bias_initialisation: 
            weight_initialisation:
        - conv:
            in_channels: 64
            num_filters: 64 
            kernel_size: 3
            stride: 1
            nonlinearity:  relu
            bias_initialisation: 
            weight_initialisation:
        - flatten:
            nonlinearity: identity
        - fc:
            in_features: 3136
            out_features: 512
            nonlinearity: relu
            bias_initialisation: 
            weight_initialisation:
        - fc:
            in_features: 512
            nonlinearity: identity
            bias_initialisation: 
            weight_initialisation:

training:
    num_episodes: 100
    test_frequency: 1
    testing: 
        # - greedy
        # - no_rep

logging:
    print_frequency: 1
    checkpoint_frequency: 500
    animation_library: imageio
    animation_file_format: mp4
    arrays:
        - [value_function, 10]
        # - [individual_value_functions, 50]
        - [value_function_std, 10]
    scalars:
        - [train_episode_reward, 1]
        - [train_episode_length, 1]
        # - [average_action_value, 50]
        # - [loss, 1]
        # - [epsilon, 1]
        - [[train_episode_reward_ensemble_runner, 4], 1]
        - [[train_episode_length_ensemble_runner, 4], 1]
        - [ensemble_episode_reward_std, 1]
        - [ensemble_episode_length_std, 1]
        - [mean_visitation_penalty, 1]
        - [mean_penalty_info, 1]
        # - [no_repeat_test_episode_reward, 1]
        # - [no_repeat_test_episode_length, 1]
        # - [cycle_count, 1]
    visualisations:
        # - [value_function, 50]
        # - [value_function_std, 50]
        # - [individual_value_functions, 50]
        # - visitation_count_heatmap
        - [individual_test_run, 200]
        - [individual_train_run, 200]
    post_visualisations:
        - value_function
        - value_function_std

post_processing:
    smoothing: 40
        
